# MMAD: The First-Ever Comprehensive Benchmark for Multimodal Large Language Models in Industrial Anomaly Detection

![VideoQA](https://img.shields.io/badge/Task-Industry_Inspection-red)
![Video-MME](https://img.shields.io/badge/Dataset-MMAD-blue)
![Gemini](https://img.shields.io/badge/Model-Gemini--1.5-green)
![GPT-4o](https://img.shields.io/badge/Model-GPT--4-green)

 Our benchmark responds to the following questions:
- How well are current MLLMs performing as industrial quality inspectors?
- Which MLLM performs the best in industrial anomaly detection? 
- What are the key challenges in industrial anomaly detection for MLLMs?

## üëÄ Overview
In the field of industrial inspection, Multimodal Large Language Models (MLLMs) have a high potential to renew the paradigms in practical applications due to their robust language capabilities and generalization abilities. However, despite their impressive problem-solving skills in many domains, MLLMs' ability in industrial anomaly detection has not been systematically studied. To bridge this gap, we present MMAD, the first-ever full-spectrum MLLMs benchmark in industrial Anomaly Detection. We defined seven key subtasks of MLLMs in industrial inspection and designed a novel pipeline to generate the MMAD dataset with 39,672 questions for 8,366 industrial images. With MMAD, we have conducted a comprehensive, quantitative evaluation of various state-of-the-art MLLMs.


<p align="center">
    <img src="./figs/overview.jpg" width="100%" height="100%">
</p>

## üìê Dataset Examples
We collected 8,366 samples from 38 classes of industrial products across 4 public datasets, generating a total of 39,672 multiple-choice questions in 7 key subtasks.
<p align="center">
    <img src="./figs/examples.jpg" width="100%" height="100%">
</p>

## üîÆ Evaluation Pipeline

### 1. Data Preparation

### 2. Model Configuration

### 3. Run Evaluation

